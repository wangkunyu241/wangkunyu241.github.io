---
title:          "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks"
date:           2025-06-01
selected:       true
pub:            "RSS"
pub_date:       "2025"
abstract: >-
  We propose Uni-NaVid, a navigation generalist that brings together multiple tasks within a single model, including Vision-and-Language Navigation, Object Navigation, Embodied Question Answering, and Human-Following. The central insight behind this unification is that different tasks create synergy beyond what specialists can achieve. On the technical side, Uni-NaVid adopts an online token merging strategy for achieving about 5 Hz model inference, and predicts action for multiple future steps for enabling non-blocking deployment in the real world.
abstract_short: >-
  We propose Uni-NaVid, a navigation generalist unifying multiple tasks in one model.
id:            "RSS_2025"
category:            "Embodied AI"
cover_gif:       /assets/images/covers/uni-navid.gif
authors:
- Jiazhao Zhang
- Kunyu Wang
- Shaoan Wang
- Minghan Li
- Haoran Liu
- Songlin Wei
- Zhongyuan Wang
- Zhizheng Zhang<sup>†</sup>
- He Wang<sup>†</sup>
links:
  Paper: https://arxiv.org/pdf/2412.06224
  Project Page: https://pku-epic.github.io/Uni-NaVid/
  Code: https://github.com/jzhzhang/Uni-NaVid
---
