---
title:          "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation"
date:           2024-04-01
selected:       true
pub:            "RSS"
pub_date:       "2024"
abstract: >-
  We propose NaVid, the first generalized embodied navigation large model. Driven by foundational Vision-Language Models and co-tuned with web-based data, NaVid transfers general-purpose knowledge to real-world navigation, greatly enhancing generalization. Moreover, NaVid relies solely on monocular video streams, encoding history as video to provide richer and more adaptive context while avoiding generalization challenges from odometer noise and map or depth discrepancies, making it easy to deploy.
cover:          /assets/images/covers/navid.jpg
cover_gif:       /assets/images/covers/navid.gif
authors:
- Jiazhao Zhang*
- Kunyu Wang*
- Rongtao Xu*
- Gengze Zhou
- Yicong Hong
- Xiaomeng Fang
- Qi Wu
- Zhizheng Zhang<sup>†</sup>
- He Wang<sup>†</sup>
links:
  Paper: https://arxiv.org/abs/2402.15852.pdf
  Project page: https://pku-epic.github.io/NaVid/
  Code: https://github.com/jzhzhang/NaVid-VLN-CE
  Oral Presentation Slides: /assets/images/covers/Navid.pdf
---
